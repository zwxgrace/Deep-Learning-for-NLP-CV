{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1975ce9a",
      "metadata": {
        "id": "1975ce9a"
      },
      "source": [
        "## Selenium presentation, context of use"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca89497",
      "metadata": {
        "id": "7ca89497"
      },
      "source": [
        ">`Selenium`  is a library that allows to control a browser (Chrome, Internet Explorer, Firefox, Safari,...) in an automatic way through a series of programs. Originally created to perform automated Web tests, this package is also used for Webscraping because of its compatibility with JavaScript. This strength makes it a real alternative to `BeautifulSoup` for dynamic Web pages, which are increasingly in the majority.\n",
        "\n",
        "> On the other hand, the use of `Selenium` creates a major constraint: the automated control of browsers requires a lot of resources, thus reducing the efficiency and the speed of execution compared to a library like `BeautifulSoup`.\n",
        "\n",
        "> The use of `Selenium` is therefore recommended (or even essential) for websites using **JavaScript** but is not recommended for retrieving **a large data load**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a92e3656",
      "metadata": {
        "id": "a92e3656"
      },
      "source": [
        "### Some introductory html notions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf29048",
      "metadata": {
        "id": "edf29048"
      },
      "source": [
        "It is useful to know the basic concepts of HTML to use Selenium effectively. In particular, here are some points to know:\n",
        "\n",
        "> * **HTML elements:** an HTML document is composed of elements nested within each other. Each element is defined by an opening and closing tag, such as <p> and </p> for a paragraph. Elements can have attributes that define additional properties, such as class or id.\n",
        "> * **The structure of an HTML document:** an HTML document is organized into a set of elements that form a hierarchy. The document has a root, which is the html element, and it can contain two main parts: head and body. The head part contains information about the document, like its title, and the body part contains the content displayed on the screen.\n",
        "> * **CSS selectors:** Selenium uses CSS selectors to find elements on a web page. A CSS selector is a string that allows you to select one or more elements based on their name, class or identifier. For example, the selector \"div.review-card\" selects all div elements that have the class review-card.\n",
        "基本选择器\n",
        "\n",
        "选择单个 HTML 元素或一类元素：\n",
        "元素选择器：直接根据 HTML 标签名选择\n",
        "示例\\: p { color: blue; } → 选择所有段落 <p>\n",
        "\n",
        "类选择器：通过 class 属性选择\n",
        "示例：.button { font-size: 16px; } → 选择所有 class=\"button\" 的元素\n",
        "\n",
        "ID选择器\\：通过 id 属性选择\n",
        "示例：#header { background-color: black; } → 选择 id=\"header\" 的唯一元素。\n",
        "By knowing these basic HTML concepts, you will be able to understand the structure of a web page and how to select the elements you want with Selenium."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce25620",
      "metadata": {
        "id": "2ce25620"
      },
      "source": [
        "## 1. Discovering and getting started with selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c824898",
      "metadata": {
        "id": "1c824898"
      },
      "source": [
        "> The first step to start scraping web sites using `Selenium` is to install the package on your virtual environment.\n",
        "\n",
        "> Run the following cell to install `selenium`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "id": "DzuM4ni7ZGSQ",
        "outputId": "9bbc066f-dfe4-460b-db0c-84a641d36fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DzuM4ni7ZGSQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.28.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288a5643",
      "metadata": {
        "id": "288a5643"
      },
      "outputs": [],
      "source": [
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "# Libraries for the last exercise (optionnal)\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import argparse\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6d85c8",
      "metadata": {
        "id": "9b6d85c8"
      },
      "source": [
        "> A **webdriver** is an essential ingredient in this process. It is what will automatically open your browser to access the website of your choice. This step is different depending on the browser you use to explore the internet. For the purpose of this class, we will use Google Chrome. For Chrome, you must first download the webdriver at https://chromedriver.chromium.org/downloads. There are several different download options depending on your version of Chrome. To find out what version of Chrome you have, click on the three vertical dots in the upper right corner of your browser window, scroll down to the help page, and select \"About Google Chrome\".\n",
        ">\n",
        "> Once chromedriver is downloaded, remember to place it at the same level as this notebook otherwise the rest of the instructions will not work.\n",
        ">\n",
        "> We can now initialize our webdriver to navigate on a page of the trustiplot site (https://fr.trustpilot.com/review/engie.fr)\n",
        "\n",
        "> **Instruction: Run the following code to set up your WebDriver.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5adc77ac",
      "metadata": {
        "id": "5adc77ac",
        "outputId": "db20b520-1580-411f-97ce-ac2f726eb71e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 12.7 kB/129 kB 10%] [Connected to cloud.r\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [62.7 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,623 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,285 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,861 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,520 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,646 kB]\n",
            "Fetched 17.4 MB in 2s (7,384 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n",
            "--2025-01-26 17:39:24--  http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 91.189.91.83, 185.125.190.83, 185.125.190.82, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|91.189.91.83|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3708 (3.6K) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libu2f-udev_1.1.4-1_all.deb’\n",
            "\n",
            "libu2f-udev_1.1.4-1 100%[===================>]   3.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-26 17:39:24 (324 MB/s) - ‘libu2f-udev_1.1.4-1_all.deb’ saved [3708/3708]\n",
            "\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "(Reading database ... 124574 files and directories currently installed.)\n",
            "Preparing to unpack libu2f-udev_1.1.4-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.4-1) ...\n",
            "Setting up libu2f-udev (1.1.4-1) ...\n",
            "--2025-01-26 17:39:25--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 173.194.194.190, 173.194.194.136, 173.194.194.93, ...\n",
            "Connecting to dl.google.com (dl.google.com)|173.194.194.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112533840 (107M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 107.32M   335MB/s    in 0.3s    \n",
            "\n",
            "2025-01-26 17:39:27 (335 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [112533840/112533840]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 124578 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (132.0.6834.110-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "--2025-01-26 17:39:41--  https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 74.125.201.207, 74.125.202.207, 74.125.69.207, ...\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|74.125.201.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7407250 (7.1M) [application/zip]\n",
            "Saving to: ‘/tmp/chromedriver_linux64.zip’\n",
            "\n",
            "chromedriver_linux6 100%[===================>]   7.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-01-26 17:39:41 (231 MB/s) - ‘/tmp/chromedriver_linux64.zip’ saved [7407250/7407250]\n",
            "\n",
            "Archive:  /tmp/chromedriver_linux64.zip\n",
            "  inflating: /tmp/chromedriver       \n",
            "  inflating: /tmp/LICENSE.chromedriver  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Set up for running selenium in Google Colab\n",
        "%%shell\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`\n",
        "wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver\n",
        "mv /tmp/chromedriver /usr/local/bin/chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromedriver-autoinstaller\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "from selenium import webdriver\n",
        "import chromedriver_autoinstaller\n",
        "\n",
        "# setup chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # ensure GUI is off\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# set path to chromedriver as per your configuration\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "# set the target URL\n",
        "url = \"https://fr.trustpilot.com/review/engie.fr\"\n",
        "\n",
        "# set up the webdriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "driver.get(url)"
      ],
      "metadata": {
        "id": "S4BY41DCZUhL",
        "outputId": "cb486817-51c3-4f4b-b13e-5ab62517264a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "S4BY41DCZUhL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from chromedriver-autoinstaller) (24.2)\n",
            "Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: chromedriver-autoinstaller\n",
            "Successfully installed chromedriver-autoinstaller-0.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2f6d80",
      "metadata": {
        "id": "ba2f6d80"
      },
      "source": [
        "> Once the driver is installed, the first step is to click on the cookies button to continue the navigation.\n",
        "> It is possible to find the path of the button by inspecting it directly:\n",
        ">\n",
        ">\n",
        "> The **`find_element`** function then allows us to search for the element using the located path. All that remains is to click on the button using the **`click`** function. Here is an example of code:\n",
        ">\n",
        ">```python\n",
        ">cookie_button = driver.find_element(By.XPATH,cookie_button_path)\n",
        ">cookie_button.click()\n",
        ">```\n",
        ">\n",
        ">  **Instruction : using the example provided, inspect the web page to find the path to the cookie \"ok\" button and click it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2414952e",
      "metadata": {
        "id": "2414952e"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578cad62",
      "metadata": {
        "id": "578cad62",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# cookie_button_path = \"/html/body/div[2]/div[2]/div/div/div[2]/div/div/button[1]\"  #change\n",
        "cookie_button_path = \"/html/body/div[3]/div[2]/div/div[1]/div/div[2]/div/button[2]\"\n",
        "# 这就是一个XPath，是一种用来定位网页中元素的路径语言，它指明了网页中某个特定按钮的位置\n",
        "# 缺点：路径长且容易受网页结构改变的影响\n",
        "# 所以代码没有用X.path 而是用的X.ID\n",
        "cookie_button = driver.find_element(By.XPATH,cookie_button_path)\n",
        "cookie_button.click()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cookie_button = driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
        "cookie_button.click()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bs-DBeZSidYd"
      },
      "id": "bs-DBeZSidYd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e92c9776",
      "metadata": {
        "id": "e92c9776"
      },
      "source": [
        "> **Instruction: start by retrieving the title of the website's first comment.**\n",
        ">\n",
        "> In the same way as for the button of validation of the cookies, it is necessary at first to inspect the web page:\n",
        ">\n",
        "> Then all that remains is to retrieve the text of the element found. Here is an example of code :\n",
        ">\n",
        "> ``title = driver.find_element(By.XPATH,title_path).text``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc4f2f2",
      "metadata": {
        "id": "4cc4f2f2"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420a5552",
      "metadata": {
        "id": "420a5552"
      },
      "outputs": [],
      "source": [
        "title_path = '//*[@id=\"__next\"]/div/div/main/div/div[4]/section/div[4]/article/div/section/div[2]/a/h2'\n",
        "\n",
        "title = driver.find_element(By.XPATH, title_path).text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa28d853",
      "metadata": {
        "id": "fa28d853"
      },
      "source": [
        "> **Instruction**\n",
        ">\n",
        "> **On the same basis:**\n",
        "> * Retrieve the body text of the first comment.\n",
        "> * Retrieve the date of the first comment.\n",
        "> * Retrieve the note of the first comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fcf0948",
      "metadata": {
        "id": "2fcf0948"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d7d0fb",
      "metadata": {
        "id": "b6d7d0fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c4c15a-d80b-4593-977d-f886bea79bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date de l'expérience: 26 janvier 2025\n",
            "Réponse en moins d’une minute un dimanche à 14h30. Personne très cordiale, qui a tout de suite compris ma demande. En mois de 5 minutes, le montant de ma mensualité d’électricité a été augmenté au montant que je souhaitais. Super prise en charge, rapide et efficace !\n",
            "Noté 5 sur 5 étoiles\n"
          ]
        }
      ],
      "source": [
        "date_path = '//*[@id=\"__next\"]/div/div/main/div/div[4]/section/div[4]/article/div/section/div[2]/p[2]'\n",
        "com_path = '//*[@id=\"__next\"]/div/div/main/div/div[4]/section/div[4]/article/div/section/div[2]/p[1]'\n",
        "mark_path = '//*[@id=\"__next\"]/div/div/main/div/div[4]/section/div[4]/article/div/section/div[1]/div[1]/img'\n",
        "\n",
        "date = driver.find_element(By.XPATH, date_path).text\n",
        "com = driver.find_element(By.XPATH, com_path).text\n",
        "mark = driver.find_element(By.XPATH, mark_path).get_attribute(\"alt\")\n",
        "\n",
        "print(date)\n",
        "print(com)\n",
        "print(mark)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "032af50f",
      "metadata": {
        "id": "032af50f"
      },
      "source": [
        "> Now that we have extracted some basic elements from the first page of the website https://fr.trustpilot.com/review/engie.fr, we would like to extract the same information but from the second page.\n",
        ">\n",
        "> First we will extract the maximum number of pages we can navigate in. We will be able to make sure that we have at least 2 pages on the site.\n",
        ">\n",
        "> To find out the maximum number of pages on the site, you can search by inspecting the button on the last page.\n",
        ">\n",
        "> **Instruction: extract the number of pages of the site https://fr.trustpilot.com/review/engie.fr and check that we have at least two pages on the site.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c6bba0",
      "metadata": {
        "id": "10c6bba0"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9def50fb",
      "metadata": {
        "id": "9def50fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ab5d92-545b-44dc-a27f-f86e6c284a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "max_page_text = driver.find_element(By.NAME, \"pagination-button-last\").text\n",
        "max_pages = int(max_page_text if max_page_text.strip() else 0)  # change\n",
        "print(max_pages > 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d45b66e",
      "metadata": {
        "id": "6d45b66e"
      },
      "source": [
        "> **Instruction: by inspecting the first page of the site https://fr.trustpilot.com/review/engie.fr, identify the location of the button to go to the next page and click on it (as previously done with the cookies button).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad24322",
      "metadata": {
        "id": "fad24322"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96dcb2d6",
      "metadata": {
        "id": "96dcb2d6"
      },
      "outputs": [],
      "source": [
        "# next_page_button_name = \"pagination-button-next\"\n",
        "# next_button = driver.find_element(By.NAME, next_page_button_name)\n",
        "\n",
        "# next_button = driver.find_element(By.ID, \"__next\")\n",
        "# next_button.click()\n",
        "\n",
        "next_button = driver.find_element(By.XPATH, \"//a[@href='/review/engie.fr?page=2']\")\n",
        "next_button.click()\n",
        "\n",
        "# next_button = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div/div/main/div/div[4]/section/div[26]/nav/a[7]')\n",
        "\n",
        "# we may have ElementClickInterceptedException that's why we are testing twice the instruction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if next_button.is_enabled():\n",
        "    next_button.click()\n",
        "else:\n",
        "    print(\"按钮不可点击\")"
      ],
      "metadata": {
        "id": "NQgTBiPcw4mS"
      },
      "id": "NQgTBiPcw4mS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    next_button.click()\n",
        "except:\n",
        "    try:\n",
        "        next_button.click()\n",
        "    except ElementClickInterceptedException:\n",
        "        pass"
      ],
      "metadata": {
        "id": "a3GkQlhtvFxU"
      },
      "id": "a3GkQlhtvFxU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e691f906",
      "metadata": {
        "id": "e691f906"
      },
      "source": [
        "> **Instruction - Once on page 2 of https://fr.trustpilot.com/review/engie.fr, as for the first page, extract the following information:**\n",
        "\n",
        "> * The title of the first comment.\n",
        "> * The content of the first comment.\n",
        "> * The date of the first comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60550d1",
      "metadata": {
        "id": "b60550d1"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d00b08",
      "metadata": {
        "id": "e8d00b08"
      },
      "outputs": [],
      "source": [
        "date_path = '//*[@id=\"__next\"]/div/div/main/div/div[4]/section/div[4]/article/div/section/div[2]/p[2]'\n",
        "com_path = '//*[@id=\"__next\"]/div/div/main/div/div[4]/section/div[4]/article/div/section/div[2]/p[1]'\n",
        "title_path = '//*[@id=\"__next\"]/div/div/main/div/div[4]/section/div[4]/article/div/section/div[2]/a/h2'\n",
        "\n",
        "date = driver.find_element(By.XPATH, date_path).text\n",
        "com = driver.find_element(By.XPATH, com_path).text\n",
        "title = driver.find_element(By.XPATH, title_path).text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef9dcbc1",
      "metadata": {
        "id": "ef9dcbc1"
      },
      "source": [
        "> **Instruction: extract the content of all the comments on page 2 of the site.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0d7d93",
      "metadata": {
        "id": "af0d7d93"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9112093c",
      "metadata": {
        "id": "9112093c"
      },
      "outputs": [],
      "source": [
        "comments_list = driver.find_elements(\n",
        "    By.XPATH,\n",
        "    '//*[contains(@class, \"typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn\")]',\n",
        ")\n",
        "comments = list(map(lambda x: x.text, comments_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comments"
      ],
      "metadata": {
        "id": "Jnibmb9ayhhO",
        "outputId": "e59838d0-b94c-4f5d-9a14-1ab8e07d4133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Jnibmb9ayhhO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "MfT7lvC-tiy5",
        "outputId": "683b0bcc-0db2-449f-cd25-a17f383b9388"
      },
      "id": "MfT7lvC-tiy5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'J’ai reçu un mail ce soir avec les nom et prénom d’un autre Client (toute la réglementation sur le RGPD pour ça !), une date de rdv fixé au 13 décembre 2023 (je rappelle que nous sommes le 21 janvier 2025…) et une proposition de rdv pour vérifier mon compteur avec potentielle facturation de plus de 40€ si ledit compteur n’était pas défaillant (bah oui, non seulement les tarifs ont augmenté de plus de 50% mais ils tentent de générer du CA en plus sur le dos des Clients… aucune limite, vraiment).\\nMa réclamation porte sur le dispositif « mon pilotage Elec » qui dysfonctionne, ce qui a été confirmé par les agents que j’ai eu en ligne (appels que j’ai enregistrés).\\nJe commence à douter serieusement du professionnalisme d’Engie !\\nVenir chez ENGIE aura été une grande erreur ! Très surpris par cet amateurisme !!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b16d4e",
      "metadata": {
        "id": "d2b16d4e"
      },
      "source": [
        "## 2. Exploitation and use of the extracted data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee436cf5",
      "metadata": {
        "id": "ee436cf5"
      },
      "source": [
        "> On this 2nd part, we will focus on a second website which is \"Avis Vérifiés\" for the same company: https://www.avis-verifies.com/avis-clients/engie-homeservices.fr\n",
        "\n",
        "> The code you will write will be to open a connection to a MySQL database, create a \"reviews\" table if it doesn't already exist, then use Selenium (as above) to open a Chrome browser and retrieve the reviews on all the available pages.\n",
        "\n",
        "> For each review, we will store the rating, the text and the date in the \"reviews\" table in the database."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c7cad2",
      "metadata": {
        "id": "78c7cad2"
      },
      "source": [
        "### Option 1 - reviews list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd39b25",
      "metadata": {
        "id": "2bd39b25"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a478739f",
      "metadata": {
        "id": "a478739f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Start the webdriver and navigate to the first page of the website\n",
        "#driver = webdriver.Chrome()\n",
        "driver.get(\"https://www.avis-verifies.com/avis-clients/engie-homeservices.fr\")\n",
        "\n",
        "reviews_list = []\n",
        "max_page = 3\n",
        "for page in range(1, max_page):\n",
        "    # Extract the review data from the current page\n",
        "    reviews_elements = driver.find_elements(By.CLASS_NAME, \"review\")\n",
        "\n",
        "    reviews_page = list(\n",
        "        map(\n",
        "            lambda x: {\n",
        "                \"rating\": x.find_element(By.CLASS_NAME, \"review__rating\").text,\n",
        "                \"text\": x.find_element(By.CLASS_NAME, \"review__text\").text,\n",
        "                \"date\": x.find_element(By.CLASS_NAME, \"review__data\").text,\n",
        "            },\n",
        "            reviews_elements,\n",
        "        )\n",
        "    )\n",
        "    reviews_list.extend(reviews_page)\n",
        "    try:\n",
        "        next_button = driver.find_element(\n",
        "            By.CLASS_NAME, 'pagination__button--next'\n",
        "        )\n",
        "        next_button.click()\n",
        "        time.sleep(5)  # for the page to load\n",
        "    except:\n",
        "        break\n",
        "\n",
        "# Close the webdriver\n",
        "# driver.quit()\n",
        "\n",
        "print(reviews_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81640680",
      "metadata": {
        "id": "81640680"
      },
      "source": [
        "### Option 2 - SQLite"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f05105",
      "metadata": {
        "id": "77f05105"
      },
      "source": [
        "> **Instruction : import the necessary libraries, create the database connection and create an empty table named reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e04955",
      "metadata": {
        "id": "c0e04955"
      },
      "outputs": [],
      "source": [
        "# import the necessary additional library\n",
        "import sqlite3\n",
        "\n",
        "# create the connection to the database\n",
        "\n",
        "db = sqlite3.connect(\"reviews.db\")\n",
        "cursor = db.cursor()\n",
        "\n",
        "cursor.execute(\"DROP TABLE IF EXISTS reviews\")\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS reviews (\n",
        "    id TEXT PRIMARY KEY,\n",
        "    rating TEXT,\n",
        "    comment TEXT,\n",
        "    date TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "db.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa214efb",
      "metadata": {
        "id": "fa214efb"
      },
      "source": [
        "> **Instruction : Go to the website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0007987e",
      "metadata": {
        "id": "0007987e"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0dedd8",
      "metadata": {
        "id": "2a0dedd8"
      },
      "outputs": [],
      "source": [
        "driver.get(\"https://www.avis-verifies.com/avis-clients/engie-homeservices.fr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc62cb5c",
      "metadata": {
        "id": "cc62cb5c"
      },
      "source": [
        "> **Instruction: we would like to extract the evaluation data (review rating, review text and review date) from all pages of the website, starting from the first page and continuing to the last page.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c17b33",
      "metadata": {
        "id": "08c17b33"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cdfaf3e",
      "metadata": {
        "id": "8cdfaf3e"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "max_page = 3\n",
        "for page in range(1, max_page):\n",
        "    # Extract the review data from the current page\n",
        "    reviews_elements = driver.find_elements(By.CLASS_NAME, \"review\")\n",
        "\n",
        "    for review in reviews_elements:\n",
        "      reviews_infos = {\n",
        "                  \"id\": str(uuid.uuid4()),\n",
        "                  \"rating\": review.find_element(By.CLASS_NAME, \"review__rating\").text,\n",
        "                  \"comment\": review.find_element(By.CLASS_NAME, \"review__text\").text,\n",
        "                  \"date\": review.find_element(By.CLASS_NAME, \"review__data\").text,\n",
        "              }\n",
        "\n",
        "      cursor.execute(\n",
        "              \"INSERT INTO reviews (id, rating, comment, date) VALUES (?, ?, ?, ?)\",\n",
        "              (reviews_infos[\"id\"], reviews_infos[\"rating\"], reviews_infos[\"comment\"], reviews_infos[\"date\"]),\n",
        "          )\n",
        "      db.commit()\n",
        "\n",
        "    try:\n",
        "        next_button = driver.find_element(\n",
        "            By.CLASS_NAME, 'pagination__button--next'\n",
        "        )\n",
        "        next_button.click()\n",
        "        time.sleep(5)  # for the page to load\n",
        "    except:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f79114",
      "metadata": {
        "id": "80f79114"
      },
      "source": [
        "This code will retrieve assessment data from all pages of the website, starting with the first page and continuing to the last page. At each iteration of the loop, it retrieves the data from the current page, inserts it into the database, and then clicks the \"next page\" button to move to the next page. When the \"next page\" button cannot be found the loop ends."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ab8fea",
      "metadata": {
        "id": "b4ab8fea"
      },
      "source": [
        "We will try to leverage on the extracted database and detect negative comments.  \n",
        ">\n",
        "> **Instruction: Develop a generic approach to detect negative comments.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21029e0",
      "metadata": {
        "id": "e21029e0"
      },
      "outputs": [],
      "source": [
        "# insert your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3e1ec4",
      "metadata": {
        "id": "bb3e1ec4"
      },
      "outputs": [],
      "source": [
        "# Select negative reviews (rating below 3)\n",
        "Q = \"SELECT * FROM reviews WHERE rating < 3\"\n",
        "cursor.execute(Q)\n",
        "\n",
        "# Retrieving the results of the query\n",
        "results = cursor.fetchall()\n",
        "\n",
        "# For each result\n",
        "for result in results:\n",
        "    # Display of the note and the text of the no\n",
        "    print(result[1])\n",
        "    print(result[2])\n",
        "\n",
        "# Close the webdriver and database connection\n",
        "driver.close()\n",
        "db.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8766dced",
      "metadata": {
        "id": "8766dced"
      },
      "source": [
        "This code opens a connection to a MySQL database, runs a query to select the negative reviews (rating below 3) from the \"reviews\" table, then displays the rating and text of each review."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48078ff3",
      "metadata": {
        "id": "48078ff3"
      },
      "source": [
        "You can modify this query to search for specific keywords in the text of the notices. For example, to search for notices containing the words \"à fuir\", you can write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b14788",
      "metadata": {
        "id": "28b14788"
      },
      "outputs": [],
      "source": [
        "Q = \"SELECT * FROM reviews WHERE review_text LIKE '%à fuir%'\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cc9ee8",
      "metadata": {
        "id": "84cc9ee8"
      },
      "source": [
        "## 3. Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2900842",
      "metadata": {
        "id": "a2900842"
      },
      "source": [
        "Beautiful Soup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. Selenium, on the other hand, is a browser automation tool that is used to automate web browsers.\n",
        "\n",
        "When used together, Selenium can be used to open a web page and interact with its contents, and then Beautiful Soup can be used to extract the desired information from the page. For example, Selenium can be used to click on a button to load more data on a page, and then Beautiful Soup can be used to extract the data that was loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd45d58b",
      "metadata": {
        "id": "fd45d58b"
      },
      "source": [
        "> Instruction\n",
        "> * Scrape all the ads of apartments for rent or sale in the city of Paris.\n",
        ">\n",
        "> * Extract the following information for each ad: the title, location, surface and price.\n",
        ">\n",
        "> * Store the extracted information in a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe06663",
      "metadata": {
        "id": "4fe06663"
      },
      "source": [
        "NB: it is possible that the site blocks your IP address if the code runs several times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21191bb9",
      "metadata": {
        "id": "21191bb9"
      },
      "outputs": [],
      "source": [
        "def parse_arguments():\n",
        "    argparser = argparse.ArgumentParser(description=\"Immo Parser arguments\")\n",
        "    argparser.add_argument(\n",
        "        \"--type\", type=str, default=\"vente\", help=\"vente ou location\"\n",
        "    )\n",
        "    argparser.add_argument(\n",
        "        \"--ville\", type=str, default=\"Paris\", help=\"ville de recherche\"\n",
        "    )\n",
        "    argparser.add_argument(\"--prix_max\", type=str, default=200000, help=\"Prix max\")\n",
        "    argparser.add_argument(\n",
        "        \"--surface_min\", type=str, default=10, help=\"surface minimale\"\n",
        "    )\n",
        "    argparser.add_argument(\n",
        "        \"--path\", type=str, default=\"immobilier.csv\", help=\"chemin vers le fichier .csv\"\n",
        "    )\n",
        "\n",
        "    argparser, _ = argparser.parse_known_args()\n",
        "\n",
        "    return argparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00270a7c",
      "metadata": {
        "id": "00270a7c"
      },
      "outputs": [],
      "source": [
        "def get_current_offers(type, ville, prix_max, surface_min):\n",
        "\n",
        "    ville = ville.lower()\n",
        "    driver = webdriver.Chrome()\n",
        "    if type == \"vente\":\n",
        "        driver.get(\"https://www.pap.fr/annonce/vente-immobiliere\")\n",
        "\n",
        "    elif type == \"location\":\n",
        "        driver.get(\"https://www.pap.fr/annonce/locations\")\n",
        "\n",
        "    assert \"PAP\" in driver.title\n",
        "    loc = driver.find_element(By.ID, \"token-input-geo_objets_ids\")\n",
        "    loc.send_keys(ville)\n",
        "    time.sleep(2)\n",
        "    loc = driver.find_element(By.ID, \"token-input-geo_objets_ids\")\n",
        "    loc.send_keys(Keys.RETURN)\n",
        "    loc.send_keys(Keys.RETURN)\n",
        "    pmax = driver.find_element(By.ID, \"prix_max\")\n",
        "    pmax.send_keys(str(prix_max))\n",
        "    smin = driver.find_element(By.ID, \"surface_min\")\n",
        "    smin.send_keys(str(surface_min))\n",
        "    smin.send_keys(Keys.RETURN)\n",
        "\n",
        "    locations = driver.find_elements(By.CLASS_NAME, \"h1\")\n",
        "    locations = [l.text.lower() for l in locations]\n",
        "\n",
        "    surfaces_elements = driver.find_elements(By.CLASS_NAME, \"item-tags\")\n",
        "    prices_elements = driver.find_elements(By.CLASS_NAME, \"item-price\")\n",
        "    links_elements = driver.find_elements(By.CLASS_NAME, \"item-title\")\n",
        "\n",
        "    surfaces, prices, links = [], [], []\n",
        "    for surf in surfaces_elements:\n",
        "        st = surf.text.split()\n",
        "        if \"m2\" in st:\n",
        "            surface_index = st.index(\"m2\") - 1\n",
        "            s = int(st[surface_index])\n",
        "        else:\n",
        "            s = None\n",
        "        surfaces.append(s)\n",
        "    for p in prices_elements:\n",
        "        ptext = p.text.split()\n",
        "        if len(ptext) > 0:\n",
        "            prices.append(int(ptext[0].replace(\".\", \"\")))\n",
        "        else:\n",
        "            prices.append(None)\n",
        "    for l in links_elements:\n",
        "        links.append(l.get_attribute(\"href\"))\n",
        "\n",
        "    driver.close()\n",
        "    df = pd.DataFrame([])\n",
        "    df[\"lieu\"] = locations\n",
        "    df[\"prix\"] = prices\n",
        "    df[\"surface\"] = surfaces\n",
        "    df[\"lien\"] = links\n",
        "    df.dropna(axis=0, inplace=True)\n",
        "    df = df[df[\"lieu\"].str.contains(ville)]\n",
        "    df[\"ratio\"] = (df[\"prix\"] / df[\"surface\"]).round()\n",
        "    df[\"date\"] = len(df) * [datetime.date.today().strftime(\"%d/%m/%Y\")]\n",
        "    df[\"type\"] = len(df) * [type]\n",
        "\n",
        "    links = df[\"lien\"]\n",
        "    df.drop(\"lien\", axis=1, inplace=True)\n",
        "\n",
        "    return df, links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1913e274",
      "metadata": {
        "id": "1913e274"
      },
      "outputs": [],
      "source": [
        "def get_additional_info(links):\n",
        "    add_df = pd.DataFrame([], columns=[\"meuble\"])\n",
        "    indic_meuble = []\n",
        "    for l in links:\n",
        "        url = l\n",
        "        r1 = requests.get(url, headers={\"User-Agent\": \"Chrome/59.0.3071.115\"})\n",
        "        coverpage = r1.content\n",
        "        soup1 = BeautifulSoup(coverpage, \"html5lib\")\n",
        "        desc = soup1.find(\"div\", class_=\"margin-bottom-30\").text\n",
        "        meuble = (\"meubl\" in desc) | (\"LMNP\" in desc)\n",
        "        indic_meuble.append(meuble)\n",
        "    add_df[\"meuble\"] = meuble\n",
        "\n",
        "    return add_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc6506e",
      "metadata": {
        "id": "ebc6506e"
      },
      "outputs": [],
      "source": [
        "def update_df_file(current_df, path):\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.append(current_df)\n",
        "    df.drop_duplicates(subset=[\"type\", \"lieu\", \"surface\", \"prix\"], inplace=True)\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    argparser = parse_arguments()\n",
        "    type = argparser.type\n",
        "    if not type in [\"location\", \"vente\"]:\n",
        "        raise ValueError(\"type doit etre location ou vente\")\n",
        "    ville = argparser.ville\n",
        "    prix_max = argparser.prix_max\n",
        "    surface_min = argparser.surface_min\n",
        "    path = argparser.path\n",
        "    current_df, links = get_current_offers(type, ville, prix_max, surface_min)\n",
        "    add_df = get_additional_info(links)\n",
        "    current_df = pd.concat((current_df, add_df))\n",
        "    update_df_file(current_df, path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "34d3564aebb0ad63b6673275ac6fa9b7422ce122acee802f48c8e7274557cb2a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}